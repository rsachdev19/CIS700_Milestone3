{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RTasmaUQsnX"
   },
   "source": [
    "# Milestone 3\n",
    "\n",
    "Stephanie Eordanidis.\n",
    "Ravjot Sachdev,\n",
    "Jackson Taber\n",
    "\n",
    "Syracuse University : College of Engineering & Computer Science\n",
    "\n",
    "223 Link Hall, Syracuse, NY 13244\n",
    "\n",
    "sleordan@syr.edu, rssachde@syr.edu, jrtaber@syr.edu\n",
    "\n",
    "CIS 700 Machine Learning and Security\n",
    "\n",
    "06/16/2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2saP1Da3Qsnf"
   },
   "source": [
    "## Theme:\n",
    " “Adversarial Text Generation: Adversarial Machine Learning Applications in Text Analysis”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M443L_l7Qsng"
   },
   "source": [
    "## Purpose:\n",
    "The purpose of this lab is to add three new GAN metrics to the project space and successfully run them on the chosen GAN models from previous milestones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsrl-QKhQsnh"
   },
   "source": [
    "## Project:\n",
    "Texygen is the name of the project selected. This project is a benchmarking tool that aids in text generation model research and testing. This tool allows for ease of various model testing to compare accuracy and synthetic data generation of models using same training baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmXaOJ8AQsni"
   },
   "source": [
    "## (Hard/Soft)ware:\n",
    "**Google Colaboratory**\t\thttps://colab.research.google.com/\n",
    "\n",
    "**GPU**                     Python 3 Google Compute Engine backend\n",
    "\n",
    "**Github**                  https://github.com/eordanis/CIS-700/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGtxblBCQsni"
   },
   "source": [
    "## Resources:\n",
    "**Original Source:** \t\thttps://github.com/geek-ai/Texygen\n",
    "\n",
    "**Modified Sources Acquired:** \t2SU Course Files Section -> Texygenmaster_Python_3.6.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kolVF4c9Qsnj"
   },
   "source": [
    "## Data:\n",
    "The data for the selected project is setup as follows:\n",
    "\n",
    "•\tGenerated data training: 5000 word and 20 sentence count\n",
    "\n",
    "•\tOracle data generation: 10,000 sentence generation\n",
    "\n",
    "•\tReal data training:\n",
    "\n",
    "\n",
    "*   image_coco : 20,000 sentences chosen from the image\n",
    " *   COCO captions data. 10,000 of which are used as training set while other 10,000 used as test set\n",
    "*   emlp_news_min : 20,000 sentences\n",
    " *  A minified version of emlp_news : reduced from 278,586 lines of text in training data to 10,000 and also 10,000 for test. Trying to run real training on 1/4 of a million lines proved too taxing even on colab pro. minified version should yeild decent results comparable to image_coco, the project default.\n",
    "*   eapoe : 266 sentences\n",
    " *  eapoe : 266 sentences Compiled from various Edgar Allan Po Poems found on referenced poem sight  [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnT5EoDoQsnk"
   },
   "source": [
    "## Modifications:\n",
    " To begin, the modified source code acquired from the 2SU application was further modified to combine the original intention of the origin source authors as well as professor modification. If no arguments are passed, all models/data are run.\n",
    " > **Note**: The order or model generation is done by first iterating over the GAN model type, then data type. If arguments are passed, those arguments will be validated and run accordingly to run a more targeted model test.\n",
    "\n",
    " Modifications were applied to eliminate much library warnings and informational messages as to keep output as clean as possible. File path naming was updated to be compliant with Google Colaboratory environment. All epoch time elapse console printing has been commented out for cleaner output reading.\n",
    "\n",
    " **Modifications under previous course works:**\n",
    "* Addition of three new GAN models\n",
    "    * CGAN\n",
    "    * InfoGAN\n",
    "    * DCGAN\n",
    "\n",
    "* Fixed LeakGAN project to work and run properly by setting standardized default flag attributes in main.py\n",
    "* Update models to save off test files for each training type run (ie oracle, cfg, real)\n",
    "* Updated models to use unified naming schema for test files, with model as the model the file was generated for and training as the training the data was generated from\n",
    "\n",
    "    **Example:** \n",
    "    * experiment-log-model-training.csv\n",
    "    * oracle-model-training.txt\n",
    "    * generator-model-training.txt\n",
    "    * test_file-model-training.txt\n",
    "\n",
    "* Updated models to set file name in main.py on the GAN model directly instead of in each model file themselves\n",
    "* Added additional field called 'log_file' to be the name for the experiment-log file data\n",
    "* Update to add new dir for midterm content vs milestone project\n",
    "* Added new util called visual.py to handle data representation\n",
    "* Allow visual.py to take directory param. if none exist, default to /content/CIS-700_clone/results\n",
    "* Updated visual.py to grab files from directory and generate visual data representations from relevant files in a more automated and less hard coded way\n",
    "* Allow main.py to take a results output directory via arg -o, if arg not present sets to results/\n",
    "* Allow main.py to take value for epoch via -p arg to use in model training\n",
    "* For argument provided training, added more detailed messaging as well as metric grid showing metrics that will be used during model generation\n",
    "* Cleaned main.py and model files to limit print to console, additionally console print is uniform and consistent across. \n",
    "* Added time elapsed pring statement for model run.\n",
    "\n",
    "**Modifications under this work:**\n",
    "* Addition of three new metrics:\n",
    "    * Time Elapse Interval (TEI)\n",
    "        TEI measures time elapse for the model per epoch run. Used to measure average time efficiency to run model for each training during epochs. Helps gauge run time during various model training components (ie pre training and adversarial training)\n",
    "    * Metric 2\n",
    "    * Metric 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlSdA9AHQsnk"
   },
   "source": [
    "## Setup:\n",
    "Due to the heft of processor/gpu usage, it was deemed necessary to run the project in the Google Colaboratory. Original attempt to run was done via Pycharm IDE Professional Edition with Anaconda derived environments, however this proved too great of a strain on the accessible workstation.\n",
    "\n",
    "Additionally, it is important to note if timeout is experienced, it is possible to run a ClickConnect script via inspector tools to prevent timeout while running long codes. The following code worked in chrome when inserted in the developer console at time of test:\n",
    "\n",
    "> \n",
    "```\n",
    "let myClick = function ClickConnect({\n",
    "console.log('Working - Preventing Timeout'); \n",
    "document.querySelector('colab-connect-button').shadowRoot.getElementById('connect').click(); \n",
    "}\n",
    "setInterval(myClick,60000);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPJON6fhRavh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8H8-PlvQsnk"
   },
   "source": [
    "### Step 1\n",
    "A new Google Colaboratory workspace was setup, titled “Milestone2”. This workspace was run using the hosted runtime environment. This document is the current document being read.\n",
    "\n",
    "In order to run against provided code base, it was necessary to sync the colab workspace the github repository files as follows\n",
    "\n",
    "```\n",
    "!git clone https://$GITHUB_AUTH@github.com/eordanis/CIS-700_clone/\n",
    "```\n",
    "\n",
    "Running this command from the first cell in the workbook syncs the drive to the github repo location of project location, as well as change to the necessary directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VW3S7VYw0c-2"
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# # to prevent nesting problems, remove directory and its contents if exists\n",
    "# #if called to remove known existing, must restart runtime before cloning again\n",
    "# dir_path = '/content/CIS-700_clone'\n",
    "# try:\n",
    "#     shutil.rmtree(dir_path)\n",
    "# except OSError as e:\n",
    "#     print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jPNtvoeQsnl"
   },
   "outputs": [],
   "source": [
    "# !git clone https://$GITHUB_AUTH@github.com/eordanis/CIS-700_clone/\n",
    "# %cd CIS-700_clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ-ge5jqQsno"
   },
   "source": [
    "### Step 2\n",
    "Now it was necessary to import and download any missing libraries the hosted colaborartoy runtime did not have readily available via the following commands:\n",
    "```\n",
    "            !pip install -r \"requirements.txt\"\n",
    "            import nltk\n",
    "            nltk.download('punkt')\n",
    "```\n",
    "Running this command from the next cell in the workbook installed the necessary libraries and at specified versions for the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MO2DlrxDQsno"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.19.0)\n",
      "Requirement already satisfied: tensorflow>=1.5.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
      "Requirement already satisfied: nltk>=3.2.3 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (3.4.5)\n",
      "Requirement already satisfied: IPython in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (7.12.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from -r requirements.txt (line 8)) (3.2.2)\n",
      "Requirement already satisfied: weasyprint in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (52.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from pandas->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2019.3)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (2.10.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.1.2)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.32.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (3.16.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (0.14.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (4.3.3)\n",
      "Requirement already satisfied: pygments in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (2.5.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (4.4.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from IPython->-r requirements.txt (line 7)) (45.2.0.post20200210)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib->-r requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib->-r requirements.txt (line 8)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from matplotlib->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: cairocffi>=0.9.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from weasyprint->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: Pillow>=4.0.0 in c:\\users\\ravsa\\appdata\\roaming\\python\\python37\\site-packages (from weasyprint->-r requirements.txt (line 9)) (7.1.2)\n",
      "Requirement already satisfied: cssselect2>=0.1 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from weasyprint->-r requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: html5lib>=0.999999999 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from weasyprint->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: Pyphen>=0.9.1 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from weasyprint->-r requirements.txt (line 9)) (0.10.0)\n",
      "Requirement already satisfied: tinycss2>=1.0.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from weasyprint->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: cffi>=0.6 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from weasyprint->-r requirements.txt (line 9)) (1.14.0)\n",
      "Requirement already satisfied: CairoSVG>=2.4.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from weasyprint->-r requirements.txt (line 9)) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.4.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->-r requirements.txt (line 7)) (0.1.8)\n",
      "Requirement already satisfied: parso>=0.5.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from jedi>=0.10->IPython->-r requirements.txt (line 7)) (0.5.2)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from traitlets>=4.2->IPython->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from cssselect2>=0.1->weasyprint->-r requirements.txt (line 9)) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from cffi>=0.6->weasyprint->-r requirements.txt (line 9)) (2.19)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from CairoSVG>=2.4.0->weasyprint->-r requirements.txt (line 9)) (0.6.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ravsa\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.5.0->-r requirements.txt (line 4)) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ravsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -r \"requirements.txt\"\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Using cached https://github.com/kpu/kenlm/archive/master.zip\n",
      "Requirement already satisfied (use --upgrade to upgrade): kenlm==0.0.0 from https://github.com/kpu/kenlm/archive/master.zip in c:\\users\\ravsa\\anaconda3\\lib\\site-packages\n",
      "Building wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (setup.py): started\n",
      "  Building wheel for kenlm (setup.py): finished with status 'done'\n",
      "  Created wheel for kenlm: filename=kenlm-0.0.0-cp37-cp37m-win_amd64.whl size=226889 sha256=12552e15da2c4cc2822bbb23d74bea6f206d62bc199df5079725f59c25719e9e\n",
      "  Stored in directory: C:\\Users\\ravsa\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-2f96jc1j\\wheels\\3d\\aa\\02\\7b4a2eab5d7a2a9391bd9680dbad6270808a147bc3b7047e4e\n",
      "Successfully built kenlm\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !wget http://kheafield.com/code/kenlm.tar.gz\n",
    "# !tar -zxvf kenlm.tar.gz\n",
    "# %cd kenlm\n",
    "# !mkdir -p build\n",
    "# %cd build\n",
    "# !cmake ..\n",
    "# !make -j 4\n",
    "%pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "# %cd ..\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rp_zaP5Qsnp"
   },
   "source": [
    "### Step 3\n",
    "Now it is time to run the application. Below are two examples of commands to run the application.\n",
    "```\n",
    "!python3 \"main.py\"\n",
    "```\n",
    "This first command was run without parameters. In the case of this command, all trainings (SeqGAN, Gsgan, TextganMmd, Leakgan, Rankgan, Maligan, Mle) were run on all available defaulted training data (oracle LSTM, real data, CFG). Running this command can take around 2+ hours to complete.\n",
    "\n",
    "```\n",
    "!python3 \"main.py\" -g seqgan -t real\n",
    "```\n",
    "\n",
    "This second command was run with parameters. In the case of this command, main was run with SeqGAN training on image_coco. Running targeted trainings take less time to run, on average completing in 5-15 minutes depending on selected parameters. With the above selection, runtime was run above 10 minutes.\n",
    "\n",
    "```\n",
    "!python3 \"main.py\" -g seqgan -t real  -d data/eapoe.txt\n",
    "```\n",
    "\n",
    "This third command was run with parameters. In the case of this command, main was run with SeqGAN training on eapoe.\n",
    "\n",
    "```\n",
    "!python3 \"main.py\" -g seqgan -t real  -d data/eapoe.txt -o results/test/\n",
    "``` \n",
    "This third command was run with parameters. In the case of this command, main was run with SeqGAN training on eapoe and results will output to results/test directory.\n",
    "\n",
    "```\n",
    "!python3 \"main.py\" -g seqgan -t real  -d data/eapoe.txt -o results/test/ -p 45\n",
    "``` \n",
    "This fourth command was run with parameters. In the case of this command, main was run with SeqGAN training on eapoe and results will output to results/test directory and run on 45 epoch for both pre and adversarial training .\n",
    "\n",
    "Running targeted trainings take less time to run, on average completing in 5-15 minutes depending on selected parameters. With the above selection, runtime was run above 10 minutes.\n",
    "\n",
    "> **NOTE:** For above estimates, based around 5 epochs. Additionally, CFG training appears to have stopped working suddenly, unsure why broken. Therefore running without that option for the time being. Additionally, the LeakGan model failed entirely to run now due to flag errors, so this model was discarded from testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGm2gigGLyRX"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9iJu3UIQsnp"
   },
   "source": [
    "### Process\n",
    "\n",
    "When running the various models, there are similar steps for each.\n",
    "1.\tBeginning Training – begin model training(s)\n",
    "2.\tSet training - sets the desired model training method\n",
    "3.\tStart model pre-train generator – uses the training data to pre-train the generator model\n",
    "4.\tStart model pre-train discriminator - – uses the training data to pre-train the discriminator model\n",
    "5.\tModel adversarial training – runs the model to generate results based on the test data and metrics applied\n",
    "6.\tFinish Training – end of model training(s)\n",
    "\n",
    "During training, each model training runs through several passes or epochs. For simplicity, base epoch is set to 5, with model training running thrice for 15 total epochs there abouts for each model trained on a particular data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbTurK39Qsnq"
   },
   "source": [
    "### Baseline Models\n",
    "\n",
    " For this report, the TextGAN and SeqGAN models were run on oracle and real training types in the previous project milestone and will be used as the baseline for new model comparisons. The real training types essentially runs the data against the image_coco.txt caption data. The TextGAN and SeqGAN was developed by the source project team to improve on existing GAN networks.\n",
    "\n",
    " With regards to TextGan, the goal of this model is to generate high quality realistic synthetic data while overcoming the convergence dilemma by using a generator that runs as a long short-term memory network and its discriminator a convolutional network. By matching high-dimension laten feature distributions of the testing and training data, this model over longer epochs has shown demonstrate a higher performance in quantitative evaluation, showing the TextGAN model can produce sentences that appear to have been written by a human, and not AI generated.\n",
    "\n",
    " For the SeqGAN model, this also proved successful in generating realistic looking sentences via this generator process. A second model was selected for comparison purposes. SeqGAN’s generator is based off the reinforcement learning stochastic policy, allowing SeqGAN to performing gradient policy update in order to circumvent differentiation issues in the generation. Its discriminator is run on complete sentences, and its results used as the reinforcement learning reward signal. According to source authors, this model boasted higher performance over others run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5Us2aDIIlQC"
   },
   "source": [
    "### New Models From Previous Milestone\n",
    "\n",
    "### **CGAN** - Conditional Generative Adversarial Network\n",
    "\n",
    "Conditional adversarial network, or CGAN for short, is a basic modification of GAN that simply adds an additional layer that conditions both the descriminator and generator model layers. For this labs purposes, an existing GAN model was used as the base, and in both the Generator and Discriminator an embedded layer was incorporated as the first layer, used as the conditioning layer. For the sake of specificity, this GAN could actually be considered an CGSGAN, however for simplicity we will refer to is as simply CGAN.\n",
    "\n",
    "### **INFOGAN**\n",
    "\n",
    "InfoGan is an adjusted simple version of GAN that seeks to maximize the mutual information of a fixed subset of GAN noise varaibles. It is able to achieve this by having numerous convolution layers added to a regular implemtation of a GAN that are connected at the end, this makes the additional cost of computation low. For the use of this lab an existing GAN was used as the base and the convolution layers were added and connected at the end. \n",
    "\n",
    "### **DCGAN**\n",
    "\n",
    "Deep convolutional generative adversarial networks (DCGANs) are a class of convolutional networks (CNNs) \n",
    "aimed to incorporate unsupervised learning. Some key components of the DCGAN architecture are the use of the Tanh activation function for the generator's output layer, LeakyReLU activation in the discriminator, and the removal of any fully connected hidden layers. With these features, the original study conducted was able to create a robust DCGAN, achieving an 82.8% accuracy on the on the CIFAR-10 image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Olz1GYmaQsnp"
   },
   "source": [
    "### Metrics\n",
    "\n",
    "**Abbreviations:**\n",
    "*   BLEU \t- BiLingual Evaluation Understudy\n",
    "*   GAN \t- Generative Adversarial Network\n",
    "*   NLL \t- Negative Log-Likelihood\n",
    "*   RL\t- Reinforcement Learning\n",
    "\n",
    "**Definitions**\n",
    "*   EmbSim – influenced by BLEU, used instead to compare the word embeddings vs BLEU’s comparison of word similarity between two sentences or documents.\n",
    "*   NLL-oracle : applied to synthetic data to determine fitting via oracle language model standards.\n",
    "*   NLL-test : dual to NLL-Oracle, used to determine a model’s capability to fit to real test data\n",
    "\n",
    "These measurement standards and more are discussed in the project directory’s “/docs/evaluation.md” location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkPFweGIKlm_"
   },
   "source": [
    "### New Metrics\n",
    "\n",
    "#### **Time Elapse Interval (TEI)**\n",
    "TEI measures time elapse for the model per epoch run. Used to measure average time efficiency to run model for each training during epochs. Helps gauge run time during various model training components (ie pre training and adversarial training)\n",
    "\n",
    "#### **Metric 2** - \n",
    "\n",
    "#### **Metric 3** - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkTxDWRTQsnq"
   },
   "source": [
    "## Test\n",
    " For this project, we will use the following arguments\n",
    "\n",
    "```\n",
    "#directory to use for model results output\n",
    "directory = '/content/CIS-700_clone/results/'\n",
    "\n",
    "#data set location to use\n",
    "data = '/content/CIS-700_clone/data/eapoe.txt'\n",
    "\n",
    "#epoch values to run for pre and adversarial training\n",
    "epoch = '5'\n",
    "```\n",
    "\n",
    " Epochs were increased left to run at 5 for the sake of time. However, it is noted that as according to original project sourcing, >= 45 epochs for the models display the best NLL loss results on epochs > 40, prior to that point results would be poorer. NLL loss values are indicated to be better the lower they are, so if these values trend downward, the models are improving. For EmbeddedSimilarity, higher values are desired for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ywwow-oCN4Ri"
   },
   "outputs": [],
   "source": [
    "#data set location to use\n",
    "data = 'data/eapoe.txt'\n",
    "\n",
    "# epoch values to run for pre and adversarial training\n",
    "epoch = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnVc8cM9OBEi"
   },
   "source": [
    "### CGAN\n",
    "The following commands are to run CGAN model on both oracle and real trainings.\n",
    "\n",
    "**NOTE:** The real data essentially trained the model on the eapoe.txt data.\n",
    "```\n",
    "!python3 \"main.py\" -g cgan -t oracle -d $data -p $epoch\n",
    "!python3 \"main.py\" -g cgan -t real -d $data -p $epoch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTE3l1-YOBEk"
   },
   "source": [
    "#### Oracle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3sJNf8gOBEl",
    "outputId": "7f7c0a1e-e5e9-4499-d744-97f3f7b8fe24",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g cgan -t oracle -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSW1_u0XOBEo"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sasXxKmiOBEq"
   },
   "source": [
    "#### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97OhChkuOBEr",
    "outputId": "de190c65-4d70-421d-db3c-ae43a199592d"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g cgan -t real -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liGvUTLYOBEs"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8dzmnc8IlQG"
   },
   "source": [
    "### DCGAN\n",
    "The following commands are to run DCGAN model on both oracle and real trainings.\n",
    "\n",
    "> **NOTE:** The real data essentially trained the model on the eapoe.txt data.\n",
    "```\n",
    "!python3 \"main.py\" -g dcgan -t oracle -d $data -p $epoch\n",
    "!python3 \"main.py\" -g dcgan -t real -d $data -p $epoch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhKxxioENQ1V"
   },
   "source": [
    "#### Oracle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QFnuHB7IlQG",
    "outputId": "8496d821-f9c1-4758-bd3a-7db0cdff0982"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g dcgan -t oracle -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsF9XI26IlQG"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCkJkRjkNT0_"
   },
   "source": [
    "#### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL13US0yIlQG",
    "outputId": "0f9443c7-34d5-4892-c077-f1c767b6e6dc"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g dcgan -t real -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me3Ufhn9IlQH"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYknSMAkQsns"
   },
   "source": [
    "### INFOGAN\n",
    "The following commands are to run INFOGAN model on both oracle and real trainings.\n",
    "\n",
    "> **NOTE:** The real data essentially trained the model on the eapoe.txt data.\n",
    "```\n",
    "!python3 \"main.py\" -g infogan -t oracle -d $data -p $epoch\n",
    "!python3 \"main.py\" -g infogan -t real -d $data -p $epoch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VecAobbhNHFH"
   },
   "source": [
    "#### Oracle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XK9faFoQsnt",
    "outputId": "9490cf88-35f1-4f0b-a118-5b8d5b8867aa"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g infogan -t oracle -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZQXB5V2Qsnt"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH2UFo8DNKpb"
   },
   "source": [
    "#### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvEehdQSQsnt",
    "outputId": "7c1b8b8b-4f81-4a05-a8b4-720d9bf7d277"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g infogan -t real -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTEmkScQQsnt"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQQn8V-8Qsnq"
   },
   "source": [
    "### SeqGAN\n",
    "The following commands are to run SeqGAN model on both oracle and real trainings.\n",
    "\n",
    "**NOTE:** The real data essentially trained the model on the eapoe.txt data.\n",
    "```\n",
    "!python3 \"main.py\" -g seqgan -t oracle -d $data -p $epoch\n",
    "!python3 \"main.py\" -g seqgan -t real -d $data -p $epoch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eE_sj9NMyNK"
   },
   "source": [
    "#### Oracle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0dEpH0EQsnr",
    "outputId": "866db06b-4df0-46b7-b85d-007ca91316d4"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g seqgan -t oracle -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sR1q74iM3Cf"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvjMNcBLQsnr"
   },
   "source": [
    "#### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Hn4zHOuQsnr",
    "outputId": "1d0150d0-9429-45ce-afaa-513cd395a898"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g seqgan -t real -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecR1k-ptQsns"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms3S31EcOCTO"
   },
   "source": [
    "### TextGAN\n",
    "The following commands are to run TextGAN model on both oracle and real trainings.\n",
    "\n",
    "**NOTE:** The real data essentially trained the model on the eapoe.txt data.\n",
    "```\n",
    "!python3 \"main.py\" -g textgan -t oracle -d $data -p $epoch\n",
    "!python3 \"main.py\" -g textgan -t real -d $data -p $epoch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ikl1wmOQOCTQ"
   },
   "source": [
    "#### Oracle Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkCGQsgjOCTR",
    "outputId": "a501844d-6b3d-4293-cf63-6216fbe335ab"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g textgan -t oracle -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkmReuM0OCTS"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kBeipbYOCTT"
   },
   "source": [
    "#### Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWU4eDLtOCTU",
    "outputId": "04fe9c55-faa1-473b-94bc-8765033a824d"
   },
   "outputs": [],
   "source": [
    "!python \"main.py\" -g textgan -t real -d $data -p $epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLb3sKxFOCTV"
   },
   "source": [
    "*For below model training and test, results are discussed later.*\n",
    "\n",
    "*For more details, see [Results Comparision](#results_comparision).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d1A0bIHf-Uc"
   },
   "source": [
    "<a id='results_comparision'></a>\n",
    "\n",
    "## Results Comparision\n",
    "\n",
    "Below sections are broken down into Data and Visual Representations of the results testing of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2szMid1RQsnu"
   },
   "source": [
    "### Data Representation\n",
    "\n",
    "Below is a display of the best values for each training of each model run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COOdCeVeRWxp",
    "outputId": "224119ef-79db-4dcf-fb80-3a545954a54a"
   },
   "outputs": [],
   "source": [
    "import utils.visual as vis\n",
    "vis.display_best_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "redoZwxfIlQH"
   },
   "source": [
    "### Visual Representation\n",
    "This sections provides a more visual representation of results and comparative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "WesgrwLAJSOJ",
    "outputId": "bd278aa7-6dc3-482f-bc08-03550b46c932"
   },
   "outputs": [],
   "source": [
    "vis.display_synth_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "DB0rVh94Skix",
    "outputId": "3eb19b4b-9313-49b3-ada5-2899c9595849"
   },
   "outputs": [],
   "source": [
    "vis.display_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLNMnekIRMnb"
   },
   "source": [
    "### Observations\n",
    "The graphs above help visualize the performances of the various GAN models with both oracle and real training. Each of our 3 models seemed to perform much better than TextGan in the NLL-Test with both Oracle and real training, as well as under the Embedding Similarity metric with Oracle training. Another interesting note is that CGan's performance seems to improve quite a bit under adversarial training under the Embedding Similarity Metric with real training, while InfoGan and TextGan seems to perform worse in the same measure. Similarly, as the number of epochs increases, CGan's performances eclipses those of the other 4 models under the NLL measure with Oracle training. Of course, these metrics are only measured across 5 epochs, so it'd be helpful to see how the models perform given more time. But we can still use this information to get a better understanding of each model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZoFeysDQsnw"
   },
   "source": [
    "## Notes\n",
    " This project was a collaborative effort by all team members listed in top of document. Project was developed/run/tested while on video call as a team effort. All parties put forth equal effort in testing, data selection and stripping, as well as understanding content.\n",
    "\n",
    " >**Note:** please refer to Team Project Documentation\\MileStone3\\Stephanie_Jackson_Ravjot_CIS700_M3_Contribution.txt for more contribution details\n",
    "\n",
    "Given the short duration of setup, running, etc there was not sufficient time to truly understand each of the models under the project. Two models from the previous milestone were used as the benchmark for comparison with the three newly incorporated models added with this milestone. However more time would be require for all encompassing tasking to really dive in and understand these models, and to run over longer epochs to see more concrete data results and comparisons. Additionally it should be noted the amount of time it takes to run these models with higher epoch values. Running the full models over and over can help training, however can take hours to complete. Furthermore, the .csv files were not populating. Given more time dedicate to this project, issues may have been able to be resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGfiU7ujQsnw"
   },
   "source": [
    "##Reference\n",
    "[1] Geek-Ai. “Texygen by Geek.AI.” GitHub, 2017, github.com/geek-ai/Texygen.\n",
    "\n",
    "[2] Yu, Lantao, et al. “SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient.” ArXiv.org, 25 Aug. 2017, arxiv.org/abs/1609.05473.\n",
    "\n",
    "[3] Zhang, Yizhe, et al. “Adversarial Feature Matching for Text Generation.” ArXiv.org, 18 Nov. 2017, arxiv.org/abs/1706.03850.\n",
    "\n",
    "[4] Chen, Xi, Duan, Yan, Houthooft, Rein, Schulman, John,\n",
    "Sutskever, Ilya, and Abbeel, Pieter. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. arXiv preprint\n",
    "arXiv:1606.03657, 2016a.\n",
    "\n",
    "[5] EA Poem Source.  https://poestories.com/read/valentine\n",
    "\n",
    "[6] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
    "https://arxiv.org/abs/1511.06434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9irmoejZQsnw"
   },
   "outputs": [],
   "source": [
    "#PDF/HTML conversion of notebook\n",
    "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
    "!pip install pypandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AAQvH7d35Rp"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='results/' --to PDF \"milestone3.ipynb\"\n",
    "!jupyter nbconvert --output-dir='results/' --to HTML \"milestone3.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6ZFU-LX7JSs"
   },
   "outputs": [],
   "source": [
    "#Download results/midterm dir\n",
    "!zip -r results.zip results/\n",
    "from google.colab import files\n",
    "files.download(\"results.zip\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "l8dzmnc8IlQG",
    "jYknSMAkQsns",
    "IQQn8V-8Qsnq",
    "Ms3S31EcOCTO"
   ],
   "name": "milestone3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
